{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90456a5d-f0b1-41c4-adb9-1cbdb2e65682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "483e3d13-afab-4915-8cbd-5ce5740a81fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 1. Reproducibility & Paths ======\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DATA_DIR = \"/home/yehoon/npz/amv/k2/k2_data\"  # (kept as-is)\n",
    "MODEL_SAVE_PATH = \"./model/PRT_Reversible_sorption/PRT_Reversible_sorption.pt\"  # (kept as-is)\n",
    "RESULT_SAVE_DIR = \"./model/PRT_Reversible_sorption/PRT_Reversible_sorption\"     # (kept as-is)\n",
    "os.makedirs(RESULT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 25\n",
    "NUM_EPOCHS = 200\n",
    "LR = 1e-3\n",
    "NX, NY = 64, 148\n",
    "TRUNK_DIM = 4      # (x, y, t_norm, dist_inlet)\n",
    "BRANCH2_DIM = 3    # (Pe, Da_a, Da_d)\n",
    "OUT_DIM = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "134b4f89-07da-458c-af7f-c3a6b1f02c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 2. Data Loader (kept paths exactly the same) ======\n",
    "def load_dataset():\n",
    "    train_dataset = torch.load(os.path.join(DATA_DIR, \"/home/yehoon/npz/k2/train_dataset_trunk4.pt\"))\n",
    "    test_dataset  = torch.load(os.path.join(DATA_DIR, \"/home/yehoon/npz/k2/test_dataset_trunk4.pt\"))\n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd64b8a2-ec99-4e9e-bdb2-444613e2a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 3. Model Definition (renamed to match paper) ======\n",
    "class BranchCNN(nn.Module):\n",
    "    \"\"\"Geometry branch: CNN encoder for (1, H, W) binary pore image.\"\"\"\n",
    "    def __init__(self, in_channels, out_dim, num_blocks=4):\n",
    "        super().__init__()\n",
    "        channels = [in_channels, 16, 32, 64, 128, 256][:num_blocks+1]\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            layers += [\n",
    "                nn.Conv2d(channels[i], channels[i+1], 3, 1, 1),\n",
    "                nn.SiLU(),\n",
    "                nn.AvgPool2d(2)\n",
    "            ]\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        h, w = NX, NY\n",
    "        for _ in range(num_blocks):\n",
    "            h //= 2\n",
    "            w //= 2\n",
    "        self.fc = nn.Linear(channels[num_blocks]*h*w, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class BranchFNN(nn.Module):\n",
    "    \"\"\"Parameter branch: FNN for (Pe, Da_a, Da_d) or similar param vector.\"\"\"\n",
    "    def __init__(self, in_dim=2, out_dim=128, hidden_dim=128, num_layers=3):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(in_dim, hidden_dim), nn.SiLU()]\n",
    "        for _ in range(num_layers-2):\n",
    "            layers += [nn.Linear(hidden_dim, hidden_dim), nn.SiLU()]\n",
    "        layers += [nn.Linear(hidden_dim, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Trunk(nn.Module):\n",
    "    \"\"\"Trunk network: FNN on (x, y, t_norm, dist_inlet) or (x, y, GDF...).\"\"\"\n",
    "    def __init__(self, trunk_in_dim, out_dim, num_layers=6, width=128):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(trunk_in_dim, width), nn.SiLU()]\n",
    "        for _ in range(num_layers-2):\n",
    "            layers += [nn.Linear(width, width), nn.SiLU()]\n",
    "        layers += [nn.Linear(width, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class PRT_DeepONet_Reversible_Sorption(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper-style DeepONet:\n",
    "      y(x) = Σ_k  BranchCNN_k(geom) * BranchFNN_k(params) * Trunk_k(query) + b\n",
    "    where:\n",
    "      - geom   : (B,1,H,W)\n",
    "      - params : (B, BRANCH2_DIM)\n",
    "      - query  : (B, 1, L, TRUNK_DIM), L = H*W (or H*W*T for transient)\n",
    "    \"\"\"\n",
    "    def __init__(self, nx=64, ny=148, trunk_in_dim=4, out_dim=128, branch2_in_dim=3, cnn_blocks=4):\n",
    "        super().__init__()\n",
    "        self.nx, self.ny = nx, ny\n",
    "        self.branch_geom = BranchCNN(1, out_dim, num_blocks=cnn_blocks)\n",
    "        self.branch_param = BranchFNN(branch2_in_dim, out_dim, hidden_dim=128, num_layers=3)\n",
    "        self.trunk = Trunk(trunk_in_dim, out_dim, num_layers=6, width=128)\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, branch1_input, branch2_input, trunk_input):\n",
    "        # trunk_input: (B, 1, L, TRUNK_DIM) or (B, L, TRUNK_DIM) — handle both\n",
    "        if trunk_input.ndim == 4:\n",
    "            trunk_input = trunk_input.squeeze(1)\n",
    "        B, L, D = trunk_input.shape\n",
    "\n",
    "        t = self.trunk(trunk_input)                 # (B, L, C)\n",
    "        t = t.unsqueeze(1)                          # (B, 1, L, C)\n",
    "        g = self.branch_geom(branch1_input).unsqueeze(1).unsqueeze(2)  # (B,1,1,C)\n",
    "        p = self.branch_param(branch2_input).unsqueeze(1).unsqueeze(2) # (B,1,1,C)\n",
    "\n",
    "        out = (g * p * t).sum(-1) + self.bias       # (B, 1, L)\n",
    "        out = out.view(B, self.nx, self.ny, 1)      # (B, H, W, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce30410-2f25-4cf7-9ad8-7666c1254acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 4. Training Utilities ======\n",
    "def train_model(\n",
    "    model, train_dataset, test_dataset, num_epochs=1000, lr=0.001, batch_size=128, patience=15):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    class EarlyStopping:\n",
    "        def __init__(self, patience=10, delta=1e-5, verbose=False):\n",
    "            self.patience = patience\n",
    "            self.delta = delta\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_loss = None\n",
    "            self.early_stop = False\n",
    "            self.best_model_state = None\n",
    "        def __call__(self, val_loss, model):\n",
    "            if self.best_loss is None or val_loss < self.best_loss - self.delta:\n",
    "                self.best_loss = val_loss\n",
    "                self.counter = 0\n",
    "                self.best_model_state = copy.deepcopy(model.state_dict())\n",
    "                if self.verbose:\n",
    "                    print(f\"Validation loss decreased. New best loss: {val_loss:.6f}\")\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.verbose:\n",
    "                    print(f\"No improvement in validation loss. Counter: {self.counter}/{self.patience}\")\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=patience, delta=1e-5, verbose=True)\n",
    "    train_losses, test_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch_branch1, batch_branch2, batch_trunk, batch_target = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                preds = model(batch_branch1, batch_branch2, batch_trunk)\n",
    "                loss = criterion(preds, batch_target)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_train_loss += loss.item()\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        running_test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                batch_branch1, batch_branch2, batch_trunk, batch_target = [b.to(device) for b in batch]\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    preds = model(batch_branch1, batch_branch2, batch_trunk)\n",
    "                    loss = criterion(preds, batch_target)\n",
    "                running_test_loss += loss.item()\n",
    "        avg_test_loss = running_test_loss / len(test_loader)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "            f\"Train Loss: {avg_train_loss:.6f}, Test Loss: {avg_test_loss:.6f}\"\n",
    "        )\n",
    "\n",
    "        early_stopping(avg_test_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered. Restoring best model state.\")\n",
    "            model.load_state_dict(early_stopping.best_model_state)\n",
    "            break\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "599dab41-c3ec-476f-8475-66bcedccd23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 5. Evaluation Example ======\n",
    "def evaluate(model, test_dataset, num_samples=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    for idx in range(min(num_samples, len(test_dataset))):\n",
    "        b1, b2, trunk, y_true = [x[idx:idx+1].to(device) for x in test_dataset.tensors]\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(b1, b2, trunk)\n",
    "        y_true_np = y_true.cpu().numpy()[0,...,0]\n",
    "        y_pred_np = y_pred.cpu().numpy()[0,...,0]\n",
    "        plt.figure(figsize=(8,3))\n",
    "        plt.subplot(1,2,1); plt.imshow(y_true_np, cmap='viridis'); plt.title('Ground Truth'); plt.axis('off')\n",
    "        plt.subplot(1,2,2); plt.imshow(y_pred_np, cmap='viridis'); plt.title('Prediction'); plt.axis('off')\n",
    "        plt.suptitle(f\"Sample #{idx}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65bdcd27-276e-41c2-8f09-4ceb2a058c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] Train Loss: 0.002001, Test Loss: 0.001098\n",
      "Validation loss decreased. New best loss: 0.001098\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m PRT_DeepONet_Reversible_Sorption(\n\u001b[1;32m     13\u001b[0m     nx\u001b[38;5;241m=\u001b[39mNX, ny\u001b[38;5;241m=\u001b[39mNY, trunk_in_dim\u001b[38;5;241m=\u001b[39mTRUNK_DIM, out_dim\u001b[38;5;241m=\u001b[39mOUT_DIM,\n\u001b[1;32m     14\u001b[0m     branch2_in_dim\u001b[38;5;241m=\u001b[39mBRANCH2_DIM, cnn_blocks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# keep your original choice\u001b[39;00m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 3) Train\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 4) Evaluate\u001b[39;00m\n\u001b[1;32m     24\u001b[0m evaluate(model, test_dataset)\n",
      "Cell \u001b[0;32mIn[5], line 48\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataset, test_dataset, num_epochs, lr, batch_size, patience)\u001b[0m\n\u001b[1;32m     46\u001b[0m     preds \u001b[38;5;241m=\u001b[39m model(batch_branch1, batch_branch2, batch_trunk)\n\u001b[1;32m     47\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(preds, batch_target)\n\u001b[0;32m---> 48\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m     50\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/miniconda3/envs/qwer/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwer/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ====== 6. Main Entry ======\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Load Data (paths left exactly as provided)\n",
    "    train_dataset, test_dataset = load_dataset()\n",
    "    # Shapes (for reference):\n",
    "    # train_dataset.tensors[0]: (N, 1, 64, 148)         # geometry\n",
    "    # train_dataset.tensors[1]: (N, 3)                  # (Pe, Da_a, Da_d)\n",
    "    # train_dataset.tensors[2]: (N, 9472, 4)            # (x, y, t_norm, dist_inlet)\n",
    "    # train_dataset.tensors[3]: (N, 64, 148, 1)         # target\n",
    "\n",
    "    # 2) Build Model (names aligned with the paper)\n",
    "    model = PRT_DeepONet_Reversible_Sorption(\n",
    "        nx=NX, ny=NY, trunk_in_dim=TRUNK_DIM, out_dim=OUT_DIM,\n",
    "        branch2_in_dim=BRANCH2_DIM, cnn_blocks=5  # keep your original choice\n",
    "    )\n",
    "\n",
    "    # 3) Train\n",
    "    train_losses, test_losses = train_model(\n",
    "        model, train_dataset, test_dataset,\n",
    "        num_epochs=NUM_EPOCHS, lr=LR, batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # 4) Evaluate\n",
    "    evaluate(model, test_dataset)\n",
    "\n",
    "    # 5) (Optional) Plot losses\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train\")\n",
    "    plt.plot(test_losses, label=\"Test\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
    "    plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d54b13c-634e-4fb1-8327-0e0b3fc2bc28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
